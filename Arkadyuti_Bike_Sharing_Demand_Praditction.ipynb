{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "3RnN4peoiCZX",
        "7hBIi_osiCS2",
        "PBTbrJXOngz2",
        "GF8Ens_Soomf",
        "g-ATYxFrGrvw",
        "-oLEiFgy-5Pf",
        "BhH2vgX9EjGr",
        "EyNgTHvd2WFk",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arka420/Arkadyuti-bike-sharing-demand-prediction-project/blob/main/Arkadyuti_Bike_Sharing_Demand_Praditction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> <h1 style=\"color:SeaGreen;font-family:verdana;\">Seoul Bike Sharing Demand Prediction(Supervised ML-Regression) </h1></b>"
      ],
      "metadata": {
        "id": "fK9xmPM64vAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<html>\n",
        "    <img src=\"https://thumbs.dreamstime.com/b/rental-bikes-ny-new-york-usa-december-citi-bike-city-us-largest-share-program-more-than-stations-across-manhattan-153214290.jpg\" alt=\"Your Image Description\" width=\"1000\" height=\"400\">\n",
        "</html>"
      ],
      "metadata": {
        "id": "QshmaCWd7c7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual - Arkadyuti Dhara\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**GitHub Link -**\n",
        "\n",
        "[Github-link](https://github.com/arka420/Arkadyuti-bike-sharing-demand-prediction-project.git)"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the contemporary urban area, where environmental concerns and traffic congestion are growing challenges, bike sharing systems have emerged as a sustainable and convenient mode of transportation. These systems allow individuals to rent bikes for short periods. However, the success and efficiency of bike sharing systems heavily rely on accurate demand prediction. This is where machine learning comes into play, offering a promising solution to forecast bike demand accurately."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Data Description </b>\n",
        "\n",
        "### <b> The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information.</b>\n",
        "\n",
        "\n",
        "### <b>Attribute Information: </b>\n",
        "\n",
        "* ### Date : year-month-day\n",
        "* ### Rented Bike count - Count of bikes rented at each hour\n",
        "* ### Hour - Hour of he day\n",
        "* ### Temperature-Temperature in Celsius\n",
        "* ### Humidity - %\n",
        "* ### Windspeed - m/s\n",
        "* ### Visibility - 10m\n",
        "* ### Dew point temperature - Celsius\n",
        "* ### Solar radiation - MJ/m2\n",
        "* ### Rainfall - mm\n",
        "* ### Snowfall - cm\n",
        "* ### Seasons - Winter, Spring, Summer, Autumn\n",
        "* ### Holiday - Holiday/No holiday\n",
        "* ### Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)"
      ],
      "metadata": {
        "id": "dE2V2GEHDZz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 2000)\n",
        "pd.set_option('display.expand_frame_repr', False)"
      ],
      "metadata": {
        "id": "G-Kp2HjyIKw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1sUhzl31jjAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the dataset\n",
        "bike_df = pd.read_csv('/content/drive/MyDrive/AlmaBetter/Bike sharing demand prediction project/dataset/SeoulBikeData.csv',encoding='latin-1')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "bike_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f' We have total {bike_df.shape[0]} rows and {bike_df.shape[1]} columns.')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "bike_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " The bike sharing demand dataset contains the following information:\n",
        " - Data Types: Date is in object format,Rented Bike Count is int, Hour is int, Temperature is float, Humidity is int,\n",
        "   Windspeed is float, Visibility is int, Dew point temperature is float,\n",
        "   Solar radiation is float, Rainfall is float, Snowfall is float.\n",
        " - Non-null Counts: All columns have non-null values, indicating no missing data.\n",
        " - Memory Usage: The bike sharing demand dataset consumes approximately 958.2 KB of memory.\n",
        "\n"
      ],
      "metadata": {
        "id": "RPkYNSf0XRKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "# checking there is any duplicate values or not\n",
        "len(bike_df[bike_df.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset has no duplicate value"
      ],
      "metadata": {
        "id": "VCPGMaD0l3vg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "bike_df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no Missing value too"
      ],
      "metadata": {
        "id": "E4_8uilMmRJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Understanding Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "bike_df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " These statistics provide an initial understanding of the data's central tendencies and variability, which can be useful for data exploration and analysis.\n"
      ],
      "metadata": {
        "id": "SEamp_BoZpvD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bike sharing demand prediction dataset typically contains several variables or features that describe various aspects of bike rental behavior. These datasets are often used for machine learning and predictive modeling tasks to forecast the demand for bike rentals based on data. Here are common variable descriptions in this dataset:\n",
        "   ### <b>Date and time column: </b>\n",
        "  - **Date**: This variable indicates the timestamp or date and time of each observation, typically in a specific format like \"YYYY-MM-DD HH:MM:SS\" or similar.\n",
        "\n",
        "### <b>Categorical columns: </b>\n",
        " - **Season**: Represents the season of the year, often categorized as Spring, Summer, Autumn and Winter.\n",
        "\n",
        " - **Holiday**: A binary variable indicating whether the day is a holiday or not.\n",
        "\n",
        " - **Functioning Day**: Another binary variable representing whether the day is a Functioning Day or a non-Functioning Day.\n",
        "\n",
        "### <b>Numerical columns: </b>\n",
        "**Weather Conditions**:\n",
        "  \n",
        "   - **Temperature**: This column represents the temperature in degrees Celsius. It gives the information about the weather conditions in terms of how hot or cold it was.\n",
        "   - **Humidity**: This column represents the relative humidity level as a percentage (%). It indicates the amount of moisture present in the air.\n",
        "   - **Windspeed**: This column represents the wind speed in meters per second (m/s). Wind speed is an important weather parameter, as strong winds can affect outdoor activities.\n",
        "   - **Rainfall**: This column represents the amount of rainfall\n",
        "     in millimeters (mm)\n",
        "   - **Solar Radiation**: This feature says solar radiation in megajoules per square meter (MJ/m2). Solar radiation measures the amount of energy received from the sun at the Earth's surface.\n",
        "   - **Snowfall**: This column represents the amount of snowfall in centimeters (cm).\n",
        "   - **Dew Point Temperature**: This column represents the dew point temperature in degrees Celsius.\n",
        "\n",
        "### <b>Target column: </b>\n",
        "- **Count of Bikes Rented**: The number of bikes rented by users.\n",
        "   "
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(bike_df.apply(lambda col:col.unique()))"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary to map old column names to new column names\n",
        "column_name_change = {'Rented Bike Count':'target_count',\n",
        "                      'Temperature(°C)':'Temperature','Humidity(%)':'Humidity',\n",
        "       'Wind speed (m/s)':'Wind_speed','Visibility (10m)':'Visibility',\n",
        "                       'Dew point temperature(°C)':'Dew_point_temperature',\n",
        "       'Solar Radiation (MJ/m2)':'Solar_radiation','Rainfall(mm)':'Rainfall',\n",
        "                       'Snowfall (cm)':'Snowfall','Functioning Day':'Functioning_day'}\n",
        "\n",
        "# Rename columns using the dictionary\n",
        "bike_df.rename(columns=column_name_change, inplace=True)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change The datatype of Date columns to extract 'Month' ,'Day', \"year\".\n",
        "bike_df['Date']=pd.DatetimeIndex(bike_df['Date'])\n",
        "\n",
        "bike_df['year'] = bike_df['Date'].dt.year\n",
        "bike_df['month'] = bike_df['Date'].dt.month_name()\n",
        "\n",
        "bike_df['day'] = bike_df['Date'].dt.day_name()"
      ],
      "metadata": {
        "id": "CoWaS4A_Svaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a new column of \"weekdays_weekend\"\n",
        "bike_df['week']=bike_df['day'].apply(lambda x : \"weekend\" if x=='Saturday' or x=='Sunday' else \"weekday\" )"
      ],
      "metadata": {
        "id": "-WQ3yqzUZ9aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Change the int64 column into catagory column\n",
        "cate_cols=['Hour','day','year','month','week','Functioning_day','Seasons','Holiday']\n",
        "for col in cate_cols:\n",
        "  bike_df[col]=bike_df[col].astype('category')"
      ],
      "metadata": {
        "id": "Z9NWl5rhRDcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.info()"
      ],
      "metadata": {
        "id": "Pq1K6MBRj3ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - I have changed the column names in DataFrame to use more readable and descriptive names.\n",
        "\n",
        " - Then Date column is object data type so I changed it to datatime data type. from here we can get all time related columns.\n",
        " - Have converted the \"Date\" column to a datetime data type, so I can access various time-related properties and columns.\n",
        " - Then, it creates three new columns ('Year', 'Month', and 'Day') and I created one more important column that is week, and it has two categories(weekend and weekday)\n",
        " - Lastly I conveterd these('Hour','month','week','Functioning_day','Seasons','Holiday) columns to categorical columns."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the columns for histplot and boxplot\n",
        "columns_to_plot = ['target_count','Temperature', 'Humidity', 'Wind_speed',\n",
        "                   'Visibility', 'Dew_point_temperature', 'Solar_radiation', 'Rainfall',\n",
        "                   'Snowfall']\n",
        "\n",
        "# Create subplots with two columns\n",
        "fig, axes = plt.subplots(nrows=len(columns_to_plot), ncols=2, figsize=(12, 20))\n",
        "\n",
        "# Loop through the columns and create histograms and boxplots\n",
        "for i, col in enumerate(columns_to_plot):\n",
        "    sns.histplot(data=bike_df, x=col, ax=axes[i, 0], kde=True, color='skyblue')\n",
        "    sns.boxplot(data=bike_df, x=col, ax=axes[i, 1], color='lightcoral')\n",
        "    axes[i, 0].set_xlabel('')  # Remove x-label for histograms\n",
        "    axes[i, 1].set_xlabel('')  # Remove x-label for boxplots\n",
        "    axes[i, 0].set_ylabel(col, fontsize=12)  # Set y-label for histograms\n",
        "\n",
        "# Adjust the layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this graph we can see distribution of numerical data. Target count has positive skewed distribution, temperature has slidely left skewed distribution,humidity has balance close to normal distribution, wind speed has positive skewed distribution, visibility,solar radiation,rainfall and snowfall are not normally distributed, dew point temperature is normally distributed,"
      ],
      "metadata": {
        "id": "XKNyDz0AucQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by the categorical column and calculate the total bike count for each category\n",
        "grouped_data = bike_df.groupby('Functioning_day')['target_count'].sum().reset_index()\n",
        "\n",
        "# Create a pie plot for 'Functioning_day'\n",
        "plt.figure(figsize=(10, 6))  # Adjust the size here\n",
        "plt.pie(grouped_data['target_count'], labels=grouped_data['Functioning_day'], autopct='%1.1f%%')\n",
        "plt.title('Functioning Day Distribution')\n",
        "plt.show()\n",
        "\n",
        "# Group the data by the 'Seasons' column and calculate the total bike count for each season\n",
        "grouped_data = bike_df.groupby('Seasons')['target_count'].sum().reset_index()\n",
        "\n",
        "# Create a pie plot for 'Seasons'\n",
        "plt.figure(figsize=(10, 6))  # Adjust the size here\n",
        "plt.pie(grouped_data['target_count'], labels=grouped_data['Seasons'], autopct='%1.1f%%')\n",
        "plt.title('Season Distribution')\n",
        "plt.show()\n",
        "\n",
        "# Group the data by the 'Holiday' column and calculate the total bike count for each category\n",
        "grouped_data = bike_df.groupby('Holiday')['target_count'].sum().reset_index()\n",
        "\n",
        "# Create a pie plot for 'Holiday'\n",
        "plt.figure(figsize=(10, 6))  # Adjust the size here\n",
        "plt.pie(grouped_data['target_count'], labels=grouped_data['Holiday'], autopct='%1.1f%%')\n",
        "plt.title('Holiday Distribution')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fvVQYac6INTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1) **Bike Rental on Functioning day**:\n",
        "   - On no-functioning day no bikes were rented.Peoples dont use reneted bikes in no functioning day.\n",
        "\n",
        "\n",
        "\n",
        "2) **Seasonal Bike Rental Patterns**:\n",
        "   - The highest bike rental counts are observed during the summer season. This indicates a strong preference for renting bikes during the warmer months. Conversely, the demand for bike rentals is considerably lower in the winter season, as expected due to colder weather conditions.\n",
        "\n",
        "3) **Bike Rentals on Non-Holidays**:\n",
        "   - The data shows that a substantial number of bikes were rented on non-holiday days. This suggests that the bike-sharing service experiences higher demand on regular working days compared to holidays.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lQ1bjDyxxRuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns = bike_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "#printing the regression plot for all the numerical features\n",
        "for col in numerical_columns:\n",
        "  fig,ax=plt.subplots(figsize=(8,6))\n",
        "  sns.regplot(x=bike_df[col],y=bike_df['target_count'],scatter_kws={\"color\": '#FFC0CB'}, line_kws={\"color\": \"#033E3E\"})"
      ],
      "metadata": {
        "id": "a37TKBr1lxsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The relationships between these weather-related numerical features and bike sharing demand are as follows: Temperature, wind speed, visibility, dew point temperature, and solar radiation have a positive influence on demand, while humidity, rainfall, and snowfall have a negative impact. These insights can help in optimizing bike sharing services based on weather conditions and forecasting demand.**"
      ],
      "metadata": {
        "id": "LOTcVSZrqX3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "cfZFaJKX8-nS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to categorize hours into custom time periods\n",
        "def categorize_time_period(Hour):\n",
        "    if 0 <= Hour < 6:\n",
        "        return 'Midnight'\n",
        "    elif 6 <= Hour < 9:\n",
        "        return 'Early Morning'\n",
        "    elif 9 <= Hour < 12:\n",
        "        return 'Morning'\n",
        "    elif 12 <= Hour < 18:\n",
        "        return 'Afternoon'\n",
        "    elif 18 <= Hour < 21:\n",
        "        return 'Evening'\n",
        "    else:\n",
        "        return 'Night'\n",
        "\n",
        "# Apply the categorization function to create a new column in the original DataFrame\n",
        "bike_df['Time_Period'] = bike_df['Hour'].apply(categorize_time_period)\n",
        "\n",
        "# Group the data by the 'Time_Period' column and calculate the total rented bike count(target count)\n",
        "grouped_data = bike_df.groupby('Time_Period')['target_count'].sum().reset_index()\n",
        "\n",
        "# Create a bar chart to visualize the rented bike demand over custom time periods\n",
        "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
        "plt.bar(grouped_data['Time_Period'], grouped_data['target_count'])\n",
        "plt.title('Rented Bike Demand by Time Period')\n",
        "plt.xlabel('Time Period')\n",
        "plt.ylabel('Rented Bike Count')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()  # Adjust the layout to prevent label clipping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m_R6sruj87Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**High demand occurs during the afternoon and evening, while demand is low during the early morning, midnight, and morning hours.**"
      ],
      "metadata": {
        "id": "HzghpUm-9KQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the seaborn style\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "\n",
        "# Create the lineplot\n",
        "plt.figure(figsize=(20, 6))  # Set the figure size\n",
        "ax = sns.barplot(data=bike_df, x='month', y='target_count', hue='year', palette=\"Set1\", linewidth=2.5)\n",
        "\n",
        "# Customize the plot\n",
        "plt.title(\"Bike Sharing Demand across month\", fontsize=16)  # Set the title and font size\n",
        "plt.xlabel(\"Month\", fontsize=12)  # Set the x-axis label and font size\n",
        "plt.ylabel(\"Bike Count\", fontsize=12)  # Set the y-axis label and font size\n",
        "plt.legend(title=\"Year\", title_fontsize=12, loc=\"upper right\")  # Customize the legend\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This bar plot clearly indicates the demand for rented bike is increasing per year. In June 2018, demand was at its highest and lowest in Januar and february. The demand for rented bikes in 2018 was significantly higher compared to the previous year.**"
      ],
      "metadata": {
        "id": "gzPZ1cB402DX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(20, 6))\n",
        "sns.pointplot(x=bike_df['Hour'], y=bike_df['target_count'], hue=bike_df['Holiday'], palette={'No Holiday': 'b', 'Holiday': 'r'})\n",
        "plt.title(\"Bike Rental Trend according to Hour on Holiday / No Holiday\")\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There is a sudden peak in bike rentals between 8 AM and 6 PM, which coincides with typical office and school/college commuting hours. This observation highlights the importance of maintaining and optimizing  bike fleet and station availability during these peak hours to ensure a positive user experience and maximize  revenue potential.**"
      ],
      "metadata": {
        "id": "gTlbisRDpU65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax=plt.subplots(figsize=(20,6))\n",
        "sns.pointplot(data=bike_df,x='Hour',y='target_count',hue='Seasons',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to seasons ')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2Ke2-zxKs0M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**During the Summer demand is highest and lowest in Winter. In Summer, when demand is high, company could charge slightly more for bike rentals. During the Winter, they could offer discounts to maintain a customer base.**"
      ],
      "metadata": {
        "id": "LUICRefzPoXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart- 8"
      ],
      "metadata": {
        "id": "8ilt21ajZ1ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "sns.lineplot(x='day', y='target_count', data=bike_df)\n",
        "plt.title('Rented Bike Count Over Days')\n",
        "plt.xlabel('Day')\n",
        "plt.ylabel('Rented Bike Count')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eBk4t3eVZ1Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Demand is higher on weekdays compared to weekends, as users only demand services during the weekdays.**"
      ],
      "metadata": {
        "id": "IHk9rmg-VJ2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "PavjPe_nBANl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by 'time_period' and calculate the average wind speed\n",
        "average_wind_speed = bike_df.groupby('Time_Period')['Wind_speed'].mean()\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(average_wind_speed.index, average_wind_speed)\n",
        "plt.xlabel('Time_Period')\n",
        "plt.ylabel('Average Wind Speed')\n",
        "plt.title('Average Wind Speed in Different Time Periods')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0vt8mPd5-rlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As we can see, there is a positive correlation between wind speed and the count of rented bike demand. We observe that wind speed tends to be higher during the afternoon and evening and demand for rented bike is also high**"
      ],
      "metadata": {
        "id": "PxbzuYwGAH4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pivot_table = bike_df.pivot_table(index=['Seasons', 'Holiday', 'Functioning_day'], values='target_count', aggfunc='count')\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(pivot_table.unstack(), cmap='YlGnBu', annot=True, fmt='d')\n",
        "plt.title('Bike Sharing Demand by Seasons, Holiday, and Functioning_day')\n",
        "plt.xlabel('Holiday')\n",
        "plt.ylabel('Seasons')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This graph showing multiple information at the same time.In Summer, specifically on non-holiday, functioning days with clear weather conditions, the demand for rented bikes is at its highest. This suggests that during the Summer season, when the weather is favorable and people are not on holiday, there is a strong preference for using the bike-sharing service, indicating a peak in bike rental activity during these conditions.**"
      ],
      "metadata": {
        "id": "wnP6NG23eOjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot with 'Temperature' on the x-axis, 'target_count' on the y-axis, and 'Hour' as color.\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.scatterplot(x='Temperature', y='target_count', hue='Hour', data=bike_df, palette='viridis')\n",
        "plt.title(\"Scatter Plot of Rented_Bike_Count vs. Temperature (Colored by Hour)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This scatter plot illustrates the relationship between bike demand and temperature. We notice that low temperatures correspond to lower demand, while temperatures between 25 to 30 degrees result in higher demand. In extremely cold weather, promoting alternative transportation services like ridesharing can provide users with practical options**"
      ],
      "metadata": {
        "id": "ytpzM-T5CHVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the figure size\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Create a correlation heatmap\n",
        "corr_matrix = bike_df.corr()\n",
        "heatmap = sns.heatmap(corr_matrix, cmap='YlGnBu', annot=True, fmt=\".2f\", linewidths=0.5)\n",
        "\n",
        "# Add a title\n",
        "plt.title(\"Correlation Heatmap\", fontsize=15)\n",
        "\n",
        "# Rotate the tick labels for better readability\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "z75WZmMcn9wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " We can see from the heatmap that \"Temperature\" and \"Dew Point Temperature\" are highly corelated. We can drop one of them.As the corelation between temperature and our dependent variable \"Bike Rented Count\" is high. So we will Keep the Temperature column and drop the \"Dew Point Temperature\" column.\n"
      ],
      "metadata": {
        "id": "EBMEWMlpU3WU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis testing is a statistical method that tests the validity of new ideas or theories against data.**"
      ],
      "metadata": {
        "id": "O6uLD5CR53tU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 1:**\n",
        "\n",
        "*Null Hypothesis (H0):* There is no significant difference in bike rental demand between different seasons (e.g., spring, summer, autumn, winter).\n",
        "\n",
        "*Alternative Hypothesis (H1):* There is a significant difference in bike rental demand between at least two seasons."
      ],
      "metadata": {
        "id": "i8rJQXNRFOmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# Assuming you have a dataset with a \"season\" column\n",
        "model = ols('target_count ~ Seasons', data=bike_df).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "alpha = 0.05  # Significance level\n",
        "p_value = anova_table['PR(>F)'][0]\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There is a significant difference between seasons.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is no significant difference between seasons.\")\n",
        "\n",
        "\n",
        "print(p_value)\n",
        "print(anova_table)"
      ],
      "metadata": {
        "id": "SVFRt0jWwSHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here I use ANOVA test because ANOVA is appropriate when we have more than two groups (in this case, four seasons) and we want to determine if there are any statistically significant differences in the means of these groups (seasons).**"
      ],
      "metadata": {
        "id": "tq5m6UqNFlza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 2:**"
      ],
      "metadata": {
        "id": "r5jHuxiAc5rU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Null Hypothesis (H0)*: There is no linear relationship between temperature and bike rental demand.**\n",
        "\n",
        "***Alternative Hypothesis (H1)*: There is a linear relationship between temperature and bike rental demand.**"
      ],
      "metadata": {
        "id": "x9wrZ-cAKWQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# Data for temperature and bike rental demand\n",
        "temperature_data = bike_df['Temperature']\n",
        "bike_rental_demand = bike_df['target_count']\n",
        "\n",
        "# Perform the Pearson correlation test\n",
        "correlation_coefficient, p_value = stats.pearsonr(temperature_data, bike_rental_demand)\n",
        "\n",
        "# Print the results\n",
        "print(\"Pearson correlation coefficient:\", correlation_coefficient)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "# Significance level (e.g., 0.05)\n",
        "alpha = 0.05\n",
        "\n",
        "# Compare the p-value to the significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There is a linear relationship between temperature and bike rental demand.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is no linear relationship between temperature and bike rental demand.\")\n"
      ],
      "metadata": {
        "id": "nvrCHbUfJOQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To test whether there is a linear relationship between two numerical variables, we can use a hypothesis test that is specifically designed for testing linear relationships. One common test for this purpose is the Pearson correlation coefficient test.\n",
        "Here we can see correlation coef is 0.53, and p-value is less than alpha so we reject null hypothesis and there is a linear relationship.**"
      ],
      "metadata": {
        "id": "XlYh68XzLJp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 3:**"
      ],
      "metadata": {
        "id": "jMtRQaXRcoig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Null Hypothesis (H0)*:  The distribution of bike sharing demand is the same for holidays and functioning days.**\n",
        "\n",
        "***Alternative Hypothesis (H1)*:  The distribution of bike sharing demand is different for holidays and functioning days.**"
      ],
      "metadata": {
        "id": "ioA2HJG-OB1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "# Create a contingency table of observed frequencies\n",
        "# Replace these numbers with your actual data\n",
        "observed_data = pd.crosstab(bike_df['Holiday'],bike_df['Functioning_day'])\n",
        "\n",
        "# Perform the Chi-Square test\n",
        "chi2, p_value, _, _ = stats.chi2_contingency(observed_data)\n",
        "\n",
        "# Print the results\n",
        "print(\"Chi-Square Statistic:\", chi2)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "# Set your significance level (e.g., 0.05)\n",
        "alpha = 0.05\n",
        "\n",
        "# Compare the p-value to the significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There is an association between holiday and functioning day with respect to bike sharing demand.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is no association between holiday and functioning day with respect to bike sharing demand.\")\n",
        "\n",
        "\n",
        "observed_data\n"
      ],
      "metadata": {
        "id": "5gnoaHqrObHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The chi-square test is used to examine the association between two categorical variables to determine if there is a statistically significant difference between the observed and expected frequencies in a contingency table. Here we can see the null hypothesis is rejected that means, the distribution of bike sharing demand is different for holidays and functioning days.\n",
        "Tt means there is a statistically significant difference in how bike sharing demand behaves differently in holiday and functioning day.**"
      ],
      "metadata": {
        "id": "rWPPhm-2ZCp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  ***Feature Selection***"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "#import the module\n",
        "#assign the 'x','y' value\n",
        "import statsmodels.api as sm\n",
        "X = bike_df[[ 'Temperature','Humidity',\n",
        "       'Wind_speed', 'Visibility','Dew_point_temperature',\n",
        "       'Solar_radiation', 'Rainfall', 'Snowfall']]\n",
        "\n",
        "Y = (bike_df['target_count'])\n",
        "X = sm.add_constant(X)\n",
        "X"
      ],
      "metadata": {
        "id": "lI8fNoTp-rYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit a OLS model\n",
        "\n",
        "model= sm.OLS(Y,X).fit()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "buIH8Zfw-rIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using ols we can select the important features and also drop some unimportant features for prediction.\n",
        "Visibility: The coefficient is -0.0097, but it is not statistically significant (P>|t| > 0.05), indicating that visibility may not have a strong impact on dependent variable.\n",
        "Dew_point_temperature: This variable's coefficient is -0.7829, but it is not statistically significant (P>|t| > 0.05), suggesting it may not be a good predictor.\n",
        "The condition number is used to check for multicollinearity. A high condition number may indicate multicollinearity among the independent variables.**\n",
        "\n",
        "**Heatmap shows the correlation between Dew_point_temperature and temperature is very high(0.91).**  "
      ],
      "metadata": {
        "id": "6UGWvbWfnotL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(bike_df.columns)"
      ],
      "metadata": {
        "id": "MRjLlWinrtkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping all the columns that do not have significant effect on target column\n",
        "columns_to_drop=['Date','Visibility', 'Dew_point_temperature','day','Time_Period']\n",
        "bike_df.drop(columns=columns_to_drop, inplace=True)"
      ],
      "metadata": {
        "id": "AdVHUWGqroef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Data Splitting***"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific proportions allocated for training and testing may differ based on individual preferences, but a common choice is to use an 80:20 split, with 80% of the data dedicated to training and 20% set aside for testing."
      ],
      "metadata": {
        "id": "kDDoe5A8uO--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign the value in x and y\n",
        "x= bike_df.drop(columns=['target_count'])\n",
        "small_constant = 1\n",
        "y =np.log10(bike_df['target_count']+small_constant)\n",
        "x.sample(5)"
      ],
      "metadata": {
        "id": "c6H9jVDUwbJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**target column is positively skewed that's why log tranfsormation is taken.**"
      ],
      "metadata": {
        "id": "Z_hcMwStwwOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "step1 = ColumnTransformer(transformers=[\n",
        "    ('col_tnf',OneHotEncoder(sparse=False,drop='first'),[0,7,8,9,10,11,12])# Encoding for categorical columns\n",
        "],remainder='passthrough')\n",
        "\n",
        "\n",
        "step2 = LinearRegression()\n",
        "\n",
        "lr_pipe = Pipeline([\n",
        "    ('step1',step1),\n",
        "    ('step2',step2)\n",
        "])\n",
        "\n",
        "# Fit the Algorithm\n",
        "lr=lr_pipe.fit(x_train,y_train)\n",
        "\n",
        "lr_y_pred = lr.predict(x_test)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lr_mae = mean_absolute_error(y_test, lr_y_pred)\n",
        "lr_mse = mean_squared_error(y_test, lr_y_pred)\n",
        "lr_rmse = np.sqrt(lr_mse)\n",
        "lr_r2 = r2_score(y_test, lr_y_pred)\n",
        "# Calculate the adjusted R-squared score\n",
        "n = x_test.shape[0]  # Number of samples\n",
        "p = x_test.shape[1]  # Number of features\n",
        "lr_adjusted_r2 = 1 - (1 -lr_r2) * ((n - 1) / (n - p - 1))"
      ],
      "metadata": {
        "id": "JKDbIUVV2e-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mean Absolute Error: {round(lr_mae, 2)}\")\n",
        "print(f\"Mean Squared Error: {round(lr_mse, 2)}\")\n",
        "print(f\"Root Mean Squared Error: {round(lr_rmse, 2)}\")\n",
        "print(f\"R-squared: {round(lr_r2, 2)}\")\n",
        "print(\"Adjusted R2:\", round(lr_adjusted_r2, 2))\n"
      ],
      "metadata": {
        "id": "tCqZPw1o2tvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The MAE, MSE, and RMSE highlight the model's accuracy and precision, while the high R-squared value indicates its ability to explain demand variations. The adjusted R-squared accounts for model complexity, Here we can see MAE,MSE,RMSE is very low, R-square and adjusted R-square value is also very good.**"
      ],
      "metadata": {
        "id": "_yRJ-dI0Bnzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  ***Cross- Validation & Hyperparameter Tuning***"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_scores = cross_val_score(lr_pipe, x_train, y_train, cv=5)  # 5-fold cross-validation\n",
        "lr_scores"
      ],
      "metadata": {
        "id": "y4i_oPgFrUQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_average_score = round(np.mean(lr_scores),3)\n",
        "lr_average_score"
      ],
      "metadata": {
        "id": "6gL7DwQavjaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The cross-validation score is slidely lower to the original R-squared value, so that the model's performance is consistent across different subsets of the data, which is a positive sign. It suggestes that our model's performance remains stable when tested on multiple data splits.**"
      ],
      "metadata": {
        "id": "0IQ6vqwGCby7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'step1__col_tnf__drop': ['first', 'if_binary'],  # Example hyperparameter for OneHotEncoder\n",
        "    'step2__fit_intercept': [True, False],  # Example hyperparameter for LinearRegression\n",
        "}\n",
        "\n",
        "# Create GridSearchCV\n",
        "grid_search = GridSearchCV(lr_pipe, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_lr_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "lr_y_pred = best_lr_model.predict(x_test)\n"
      ],
      "metadata": {
        "id": "qlGSEJ_iG-2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(y_test, lr_y_pred)\n",
        "rmse = np.sqrt(mse)  # RMSE is the square root of MSE\n",
        "mae = mean_absolute_error(y_test, lr_y_pred)\n",
        "r_squared = r2_score(y_test, lr_y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"R-squared (R²): {r_squared}\")\n"
      ],
      "metadata": {
        "id": "GJqD5NtOHTQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GridSearchCV is a popular choice for hyperparameter tuning in linear regression . It helps to find the best set of hyperparameters to optimize model's performance. Here we have same result, there is no improvement in R square.**"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lr_y_pred: A list containing your predicted values\n",
        "# y_test: containing your actual values\n",
        "\n",
        "# Create a figure with a specified size\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot the predicted values as one line\n",
        "plt.plot(lr_y_pred)\n",
        "\n",
        "# Plot the actual values as another line\n",
        "plt.plot(np.array(y_test))\n",
        "\n",
        "# Add a legend to distinguish between the predicted and actual lines\n",
        "plt.legend([\"Predicted\", \"Actual\"])\n",
        "\n",
        "# Label the x-axis\n",
        "plt.xlabel('No of Test Data')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "whC0qsVKYIFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function to plot scatter plot for y_test and y_actual.\n",
        "def plot_scatter(y_pred,y_test):\n",
        "  '''Plot scatter plot for y_test values and\n",
        "  y_test values. To check how close we are to regresson line'''\n",
        "  plt.figure(figsize=(16,5))\n",
        "  sns.regplot(x=y_test,y=y_pred,scatter_kws={'color':'#6082B6'},line_kws={'color':'#34495E'})\n",
        "  plt.xlabel('Actual')\n",
        "  plt.ylabel(\"Predicted\")\n",
        "  plt.title(\"Actual v/s Predicted\")\n"
      ],
      "metadata": {
        "id": "wyCy6WHr2wrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how predicted values and actual values are close  to the regression line\n",
        "plot_scatter(lr_y_pred,y_test)"
      ],
      "metadata": {
        "id": "OIEfmyq92Sbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ridge Regression"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step1 = ColumnTransformer(transformers=[\n",
        "    ('col_tnf', OneHotEncoder(sparse=False, drop='first'), [0,7,8,9,10,11,12])# Encoding for categorical columns\n",
        "], remainder='passthrough')\n",
        "\n",
        "step2 = Ridge(alpha=10)\n",
        "\n",
        "rid = Pipeline([\n",
        "    ('step1', step1),\n",
        "    ('step2', step2)\n",
        "])\n",
        "\n",
        "rid.fit(x_train, y_train)\n",
        "\n",
        "rid_y_pred = rid.predict(x_test)"
      ],
      "metadata": {
        "id": "TNzoQaS40FZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rid_mae = mean_absolute_error(y_test, rid_y_pred)\n",
        "rid_mse = mean_squared_error(y_test, rid_y_pred)\n",
        "rid_rmse = np.sqrt(rid_mse)\n",
        "rid_r2 = r2_score(y_test, rid_y_pred)\n",
        "# Calculate the adjusted R-squared score\n",
        "n = x_test.shape[0]  # Number of samples\n",
        "p = x_test.shape[1]  # Number of features\n",
        "rid_adjusted_r2 = 1 - (1 -rid_r2) * ((n - 1) / (n - p - 1))"
      ],
      "metadata": {
        "id": "vqlCH9VIldG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mean Absolute Error: {round(rid_mae, 2)}\")\n",
        "print(f\"Mean Squared Error: {round(rid_mse, 2)}\")\n",
        "print(f\"Root Mean Squared Error: {round(rid_rmse, 2)}\")\n",
        "print(f\"R-squared: {round(rid_r2, 3)}\")\n",
        "print(\"Adjusted R2:\", round(rid_adjusted_r2, 3))\n"
      ],
      "metadata": {
        "id": "xkLIFWbNl5do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  ***Cross- Validation & Hyperparameter Tuning***"
      ],
      "metadata": {
        "id": "kbeh_ii2e5Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rid_scores = cross_val_score(rid, x_train, y_train, cv=5)  # 5-fold cross-validation\n",
        "rid_scores"
      ],
      "metadata": {
        "id": "3GeS9gusgV2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rid_average_score = round(np.mean(rid_scores),3)\n",
        "rid_average_score"
      ],
      "metadata": {
        "id": "i3xZEPnWgirc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The cross-validation score is also slidely lower to the original R-squared value, so that the model's performance is consistent across different subsets of the data, which is a positive sign.**"
      ],
      "metadata": {
        "id": "_CkhR5qkhTcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a parameter grid for Ridge alpha values\n",
        "param_grid = {'step2__alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,60,100,0.5,1.5,1.6,1.7,1.8,1.9]}\n",
        "\n",
        "# Use the same pipeline as defined earlier\n",
        "ridge_pipeline = Pipeline([\n",
        "    ('step1', step1),\n",
        "    ('step2', Ridge())  # Use the default alpha here\n",
        "])\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "ridge_grid_search = GridSearchCV(ridge_pipeline, param_grid, cv=5)\n",
        "ridge_grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_alpha = ridge_grid_search.best_params_['step2__alpha']\n",
        "print(f' best alpha value is {best_alpha}')"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step1 = ColumnTransformer(transformers=[\n",
        "    ('col_tnf', OneHotEncoder(sparse=False, drop='first'), [0,7,8,9,10,11,12])# Encoding for categorical columns\n",
        "], remainder='passthrough')\n",
        "\n",
        "step2 = Ridge(alpha=0.5)\n",
        "\n",
        "rid=pipe = Pipeline([\n",
        "    ('step1', step1),\n",
        "    ('step2', step2)\n",
        "])\n",
        "\n",
        "rid.fit(x_train, y_train)\n",
        "\n",
        "rid_y_pred = rid.predict(x_test)"
      ],
      "metadata": {
        "id": "uoQLY6ek7ja5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rid_mae = mean_absolute_error(y_test, rid_y_pred)\n",
        "rid_mse = mean_squared_error(y_test, rid_y_pred)\n",
        "rid_rmse = np.sqrt(rid_mse)\n",
        "rid_r2 = r2_score(y_test, rid_y_pred)\n",
        "# Calculate the adjusted R-squared score\n",
        "n = x_test.shape[0]  # Number of samples\n",
        "p = x_test.shape[1]  # Number of features\n",
        "rid_adjusted_r2 = 1 - (1 -rid_r2) * ((n - 1) / (n - p - 1))"
      ],
      "metadata": {
        "id": "tG9u5hw67xiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mean Absolute Error: {round(rid_mae, 2)}\")\n",
        "print(f\"Mean Squared Error: {round(rid_mse, 2)}\")\n",
        "print(f\"Root Mean Squared Error: {round(rid_rmse, 2)}\")\n",
        "print(f\"R-squared: {round(rid_r2, 3)}\")\n",
        "print(\"Adjusted R2:\", round(rid_adjusted_r2, 3))"
      ],
      "metadata": {
        "id": "amMUG_0T7zV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Slide improvement in evaluation matrics**"
      ],
      "metadata": {
        "id": "9QcacTG7DUBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(rid_y_pred)\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel('No of Test Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H-ebbrtIpGtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how predicted values and actual values are close  to the regression line\n",
        "plot_scatter(rid_y_pred,y_test)"
      ],
      "metadata": {
        "id": "xRKdYM9I2-WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lasso Regression"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step1 = ColumnTransformer(transformers=[\n",
        "    ('col_tnf',OneHotEncoder(sparse=False,drop='first'),[0,7,8,9,10,11,12])# Encoding for categorical columns\n",
        "],remainder='passthrough')\n",
        "\n",
        "step2 = Lasso(alpha=0.001)\n",
        "\n",
        "las= Pipeline([\n",
        "    ('step1',step1),\n",
        "    ('step2',step2)\n",
        "])\n",
        "\n",
        "las.fit(x_train,y_train)\n",
        "\n",
        "las_y_pred = las.predict(x_test)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "las_mae = mean_absolute_error(y_test, las_y_pred)\n",
        "las_mse = mean_squared_error(y_test, las_y_pred)\n",
        "las_rmse = np.sqrt(las_mse)\n",
        "las_r2 = r2_score(y_test, las_y_pred)\n",
        "# Calculate the adjusted R-squared score\n",
        "n = x_test.shape[0]  # Number of samples\n",
        "p = x_test.shape[1]  # Number of features\n",
        "las_adjusted_r2 = 1 - (1 -las_r2) * ((n - 1) / (n - p - 1))"
      ],
      "metadata": {
        "id": "s5Oh-hBtl1mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mean Absolute Error: {las_mae}\")\n",
        "print(f\"Mean Squared Error: {las_mse}\")\n",
        "print(f\"Root Mean Squared Error: {las_rmse}\")\n",
        "print(f\"R-squared: {las_r2}\")\n",
        "print(\"Adjusted R2:\", las_adjusted_r2)"
      ],
      "metadata": {
        "id": "G4v2Hi62l1Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Hyperparameter Tunning**"
      ],
      "metadata": {
        "id": "_Ud40sywfV9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid for Lasso\n",
        "lasso_param_grid = {\n",
        "    'step2__alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100,0.0014]  # Adjust the values as needed\n",
        "}\n",
        "\n",
        "# Create the pipeline\n",
        "step1 = ColumnTransformer(transformers=[\n",
        "    ('col_tnf', OneHotEncoder(sparse=False, drop='first'), [0, 7, 8, 9, 10, 11, 12])  # Encoding for categorical columns\n",
        "], remainder='passthrough')\n",
        "\n",
        "step2 = Lasso(alpha=0.001)\n",
        "\n",
        "las = Pipeline([\n",
        "    ('step1', step1),\n",
        "    ('step2', step2)\n",
        "])\n",
        "\n",
        "# Create GridSearchCV object for hyperparameter tuning\n",
        "lasso_grid_search = GridSearchCV(las, lasso_param_grid, cv=5)\n",
        "\n",
        "# Fit the model with grid search\n",
        "lasso_grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_alpha = lasso_grid_search.best_params_['step2__alpha']\n",
        "\n",
        "# Fit the Lasso model with the best hyperparameters\n",
        "best_lasso_model = Pipeline([\n",
        "    ('step1', step1),\n",
        "    ('step2', Lasso(alpha=best_alpha))\n",
        "])\n",
        "best_lasso_model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "las_y_pred = best_lasso_model.predict(x_test)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Alpha for Lasso:\", best_alpha)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate predictions\n",
        "las_y_pred = best_lasso_model.predict(x_test)\n",
        "\n",
        "\n",
        "# Calculate R-squared (R²) - Coefficient of Determination\n",
        "r2 = r2_score(y_test, las_y_pred)\n",
        "# Calculate Mean Absolute Error (MAE)\n",
        "las_mae = mean_absolute_error(y_test, las_y_pred)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "las_mse = mean_squared_error(y_test, las_y_pred)\n",
        "\n",
        "# Calculate Root Mean Squared Error (RMSE)\n",
        "las_rmse = np.sqrt(las_mse)\n",
        "# Calculate R-squared (R²) - Coefficient of Determination\n",
        "las_r2 = r2_score(y_test, las_y_pred)\n",
        "\n",
        "# Calculate the adjusted R-squared score\n",
        "n = x_test.shape[0]  # Number of samples\n",
        "p = x_test.shape[1]  # Number of features\n",
        "las_adjusted_r2 = 1 - (1 -las_r2) * ((n - 1) / (n - p - 1))\n",
        "\n",
        "# Print the metrics\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"R-squared (R²):\", r2)\n"
      ],
      "metadata": {
        "id": "_75Qp_v2-U22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# las_y_pred: A list containing predicted values\n",
        "# y_test: A list or numpy array containing actual values\n",
        "\n",
        "# Create a figure with a specified size\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot the predicted values as one line\n",
        "plt.plot(las_y_pred)\n",
        "\n",
        "# Plot the actual values as another line\n",
        "plt.plot(np.array(y_test))\n",
        "\n",
        "# Add a legend to distinguish between the predicted and actual lines\n",
        "plt.legend([\"Predicted\", \"Actual\"])\n",
        "\n",
        "# Label the x-axis\n",
        "plt.xlabel('No of Test Data')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H22YxPHqXX2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how predicted values and actual values are close  to the regression line\n",
        "plot_scatter(las_y_pred,y_test)"
      ],
      "metadata": {
        "id": "F7a2IsEJ3LAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "W_46uDWWzxXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step1 = ColumnTransformer(transformers=[\n",
        "    ('col_tnf',OneHotEncoder(sparse=False,drop='first'),[0,7,8,9,10,11,12])# Encoding for categorical columns\n",
        "],remainder='passthrough')\n",
        "\n",
        "step2 = RandomForestRegressor(n_estimators=500,\n",
        "                              random_state=5,\n",
        "                              max_samples=0.5,\n",
        "                              max_features=0.75,\n",
        "                              max_depth=15)\n",
        "\n",
        "rf = Pipeline([\n",
        "    ('step1',step1),\n",
        "    ('step2',step2)\n",
        "])\n",
        "\n",
        "rf.fit(x_train,y_train)\n",
        "\n",
        "rf_y_pred = rf.predict(x_test)"
      ],
      "metadata": {
        "id": "ucG48m_-zukl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rf_mae = mean_absolute_error(y_test, rf_y_pred)\n",
        "rf_mse = mean_squared_error(y_test, rf_y_pred)\n",
        "rf_rmse = np.sqrt(rf_mse)\n",
        "rf_r2 = r2_score(y_test, rf_y_pred)\n",
        "# Calculate the adjusted R-squared score\n",
        "n = x_test.shape[0]  # Number of samples\n",
        "p = x_test.shape[1]  # Number of features\n",
        "rf_adjusted_r2 = 1 - (1 -rf_r2) * ((n - 1) / (n - p - 1))"
      ],
      "metadata": {
        "id": "5efS7IVMmkKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mean Absolute Error: {round(rf_mae, 2)}\")\n",
        "print(f\"Mean Squared Error: {round(rf_mse, 2)}\")\n",
        "print(f\"Root Mean Squared Error: {round(rf_rmse, 2)}\")\n",
        "print(f\"R-squared: {round(rf_r2, 4)}\")\n",
        "print(\"Adjusted R2:\", round(rf_adjusted_r2, 4))\n"
      ],
      "metadata": {
        "id": "wgqRGi_6mnbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_scores = cross_val_score(rf, x_train, y_train, cv=5)  # 5-fold cross-validation\n",
        "rf_scores"
      ],
      "metadata": {
        "id": "201HEk45yrlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_average_score = round(np.mean(rf_scores),3)\n",
        "rf_average_score"
      ],
      "metadata": {
        "id": "KLjOUU_LzPi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The cross-validation scores are very close to each other and close to the training scores, it indicates that the model is not overfitting.**"
      ],
      "metadata": {
        "id": "WNhuFgJl8sLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid\n",
        "n_estimators = [60, 80, 100]\n",
        "max_depth = [15, 20]\n",
        "max_leaf_nodes = [40, 60, 80]\n",
        "\n",
        "param_grid = {\n",
        "    'step2__n_estimators': n_estimators,\n",
        "    'step2__max_depth': max_depth,\n",
        "    'step2__max_leaf_nodes': max_leaf_nodes\n",
        "}\n",
        "\n",
        "# Create the pipeline\n",
        "step1 = ColumnTransformer(transformers=[\n",
        "    ('col_tnf', OneHotEncoder(sparse=False, drop='first'), [0, 7, 8, 9, 10, 11, 12])\n",
        "], remainder='passthrough')\n",
        "\n",
        "step2 = RandomForestRegressor(n_estimators=500, random_state=5, max_samples=0.5, max_features=0.75, max_depth=15)\n",
        "\n",
        "rf1 = Pipeline([\n",
        "    ('step1', step1),\n",
        "    ('step2', step2)\n",
        "])\n",
        "\n",
        "# Create GridSearchCV object for hyperparameter tuning\n",
        "grid_search = GridSearchCV(rf1, param_grid, cv=5)\n",
        "\n",
        "# Fit the model with grid search\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and the best estimator\n",
        "best_params = grid_search.best_params_\n",
        "best_estimator = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the best estimator\n",
        "rf_y_pred = best_estimator.predict(x_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(y_test, rf_y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, rf_y_pred)\n",
        "r2 = r2_score(y_test, rf_y_pred)\n",
        "\n",
        "# Print the metrics and best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"R-squared (R²):\", r2)\n"
      ],
      "metadata": {
        "id": "CZDorDPeJV-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The original model had a slightly better fit to the data compared to the GridSearchCV-tuned model. In general, a slight decrease in R-squared when performing hyperparameter tuning doesn't necessarily indicate overfitting**"
      ],
      "metadata": {
        "id": "1DvjRonCLR5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# rf_y_pred: A list containing predicted values from a random forest regression model\n",
        "# y_test: containing actual values\n",
        "\n",
        "# Create a figure with a specified size\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot the predicted values as one line\n",
        "plt.plot(rf_y_pred)\n",
        "\n",
        "# Plot the actual values as another line\n",
        "plt.plot(np.array(y_test))\n",
        "\n",
        "# Add a legend to distinguish between the predicted and actual lines\n",
        "plt.legend([\"Predicted\", \"Actual\"])\n",
        "\n",
        "# Label the x-axis\n",
        "plt.xlabel('No of Test Data')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rwHE5Sx4Ycqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how predicted values and actual values are close  to the regression line\n",
        "plot_scatter(rf_y_pred,y_test)"
      ],
      "metadata": {
        "id": "tOn9tgQW3VF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation matrics comparision**"
      ],
      "metadata": {
        "id": "iFkPZuSipPf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the metrics and models\n",
        "metrics = [\"MAE\", \"MSE\", \"RMSE\", \"R-squared\", \"Adjusted R2\"]\n",
        "linear_reg_metrics = [lr_mae, lr_mse, lr_rmse, lr_r2, lr_adjusted_r2]\n",
        "ridge_reg_metrics = [rid_mae, rid_mse, rid_rmse, rid_r2, rid_adjusted_r2]\n",
        "lasso_reg_metrics = [las_mae, las_mse, las_rmse, las_r2, las_adjusted_r2]\n",
        "random_forest_metrics = [rf_mae, rf_mse, rf_rmse, rf_r2, rf_adjusted_r2]\n",
        "\n",
        "# Create a bar chart\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "width = 0.2\n",
        "\n",
        "x = range(len(metrics))\n",
        "plt.bar(x, linear_reg_metrics, width, label=\"Linear Regression\", align=\"center\")\n",
        "plt.bar([i + width for i in x], ridge_reg_metrics, width, label=\"Ridge Regression\", align=\"center\")\n",
        "plt.bar([i + 2 * width for i in x], lasso_reg_metrics, width, label=\"Lasso Regression\", align=\"center\")\n",
        "plt.bar([i + 3 * width for i in x], random_forest_metrics, width, label=\"Random Forest\", align=\"center\")\n",
        "\n",
        "ax.set_xticks([i + 1.5 * width for i in x])\n",
        "ax.set_xticklabels(metrics)\n",
        "plt.xlabel(\"Evaluation Metrics\")\n",
        "plt.title(\"Comparison of Evaluation Metrics for Different Models\")\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9ZrOet5dpO_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of the final prediction model depends on the specific goals and requirements of the project, as well as considerations for model interpretability and computational complexity. However, based on the evaluation metrics for the Random Forest Regressor, it seems like a strong candidate for the final prediction model.\n",
        "\n",
        "Here's a brief analysis of the evaluation metrics:\n",
        "\n",
        "1. **Mean Absolute Error (MAE)**: A low MAE of 0.121 suggests that, on average,  model's predictions are very close to the actual values. This is a good indication of the model's accuracy.\n",
        "\n",
        "2. **Mean Squared Error (MSE)**: A low MSE of 0.0342 also suggests that the model's predictions are accurate and relatively consistent.\n",
        "\n",
        "3. **Root Mean Squared Error (RMSE)**: An RMSE of 0.1849 indicates that the model's predictions have small errors on average, which is a positive sign.\n",
        "\n",
        "4. **R-squared (R²)**: An R-squared value of 0.9291 is quite high, meaning that  model explains a significant portion of the variance in the target variable. This suggests that the model is a good fit for data.\n",
        "\n",
        "5. **Adjusted R-squared**: The adjusted R-squared value is also high, indicating that  model is well-fitted to the data without overfitting.\n",
        "\n",
        "The Random Forest Regressor has provided excellent results and it's a strong choice for the final prediction model due to its strong predictive performance, robustness to overfitting, and ability to capture complex relationships in the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "J25pUKCdPNnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importances\n",
        "feature_importances = rf.named_steps['step2'].feature_importances_\n",
        "\n",
        "# Assuming you have the column names (feature names) from step1\n",
        "feature_names = rf.named_steps['step1'].get_feature_names_out(input_features=x_train.columns)\n",
        "\n",
        "# Create a DataFrame to display the feature importances with their corresponding feature names\n",
        "importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "\n",
        "# Sort the DataFrame by importance in descending order\n",
        "importances_df = importances_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display the top N important features (e.g., top 10 features)\n",
        "top_n = 20\n",
        "print(importances_df.head(top_n))\n"
      ],
      "metadata": {
        "id": "5cctO5Yb9-Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the DataFrame by importance in ascending order\n",
        "importances_df = importances_df.sort_values(by='Importance', ascending=True)\n",
        "\n",
        "# Display the top N important features (e.g., top 10 features)\n",
        "top_n = 20\n",
        "top_features = importances_df.tail(top_n)  # Use .tail() to get the top features in ascending order\n",
        "\n",
        "# Create a horizontal bar chart using Plotly\n",
        "fig = px.bar(top_features, x='Importance', y='Feature', orientation='h', title=f'Top {top_n} Important Features')\n",
        "\n",
        "# Customize the layout (optional)\n",
        "fig.update_layout(\n",
        "    xaxis_title='Importance',\n",
        "    yaxis_title='',\n",
        "    xaxis=dict(tickformat='%'),\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "OmZnEzY7A7Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The feature importance scores represent the contribution of each feature to the predictions made by Random Forest Regressor model for bike sharing demand prediction. These scores indicate how much each feature influences the model's output. Here's an explanation for the feature importance results:\n",
        "\n",
        "1. **col_tnf__Functioning_day_Yes (0.482)**: This feature appears to have the highest importance. It suggests that whether it is a functioning day or not has the most significant impact on bike sharing demand according to model.\n",
        "\n",
        "2. **remainder__Temperature (0.166)**: Temperature is the second most important feature. It indicates that temperature plays a substantial role in predicting bike demand.\n",
        "\n",
        "3. **remainder__Humidity (0.087)**: Humidity is also an essential factor in predicting bike sharing demand, although it has a slightly lower importance than temperature.\n",
        "\n",
        "4. **remainder__Rainfall (0.069)**: Rainfall is another significant feature. It suggests that rainy conditions can affect bike demand.\n",
        "\n",
        "5. **col_tnf__Seasons_Winter (0.030)**: The season of winter has moderate importance in predicting bike demand. Seasonal variations are often crucial in this type of prediction.\n",
        "\n",
        "6. **remainder__Solar_radiation (0.026)**: Solar radiation is moderately important. It may indicate that sunny or cloudy conditions impact bike sharing demand.\n",
        "\n",
        "7. **col_tnf__Hour_4 (0.024)**: The fourth hour of the day is more important than other hours. It might suggest that early morning has a specific influence.\n",
        "\n",
        "8. **col_tnf__Hour_5 (0.022)**: The fifth hour of the day also holds some importance, possibly indicating the start of the workday.\n",
        "\n",
        "9. **col_tnf__Hour_3 (0.014)**: The third hour of the day has a moderate impact. It might correspond to the early morning hours.\n",
        "\n",
        "10. **remainder__Wind_speed (0.010)**: Wind speed plays a minor role but still contributes to predictions.\n",
        "\n",
        "These feature importance scores are essential in understanding which features have the most significant impact on bike sharing demand prediction model. We can use this information to identify the key factors that influence bike demand and potentially focus on these factors."
      ],
      "metadata": {
        "id": "O4O3vPi2NC9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train a model to predict the number of rented bike count in given weather conditions. First, we do Exploratory Data Analysis on the data set. We look for null values that is not found in dataset. We also perform correlation and ols analysis to extract out the important and relevant feature. The target variable had some outliers, so a log transformation has been applied to remove them.\n",
        "\n",
        "Next we implemented few machine learning algorithms like Linear Regression,lasso and ridge regression and Random forest regressor. we calculated evaluation matrix to check model accuracy. model appears to be performing well, with low errors (MAE, MSE, RMSE) and a high percentage of variance explained (R2 and Adjusted R2)\n",
        "\n",
        "•\tSeason: We see highest number bike rentals in Summer Seasons and the lowest in Spring season.\n",
        "\n",
        "•\tWeather: As one would expect, we see highest number of bike rentals on a clear day and the lowest on a snowy or rainy day.\n",
        "\n",
        "•\tHumidity: With increasing humidity, we see decrease in the number of bike rental count.\n",
        "\n",
        "•  High demand of bike at 8AM as the time of people for going to their work and 6PM time of returning to home.\n",
        "• Demand is high during afternoon and evening.\n",
        "• Random forest regressor works best as we get r-squared 93%\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "However, this is not the ultimate end. As this data is time dependent, the values for variables like temperature, windspeed, solar radiation etc., will not always be consistent. Therefore, there will be scenarios where the model might not perform well. As Machine learning is an exponentially evolving field, we will have to be prepared for all contingencies and also keep checking our model from time to time."
      ],
      "metadata": {
        "id": "sL7nrxkFlgHT"
      }
    }
  ]
}